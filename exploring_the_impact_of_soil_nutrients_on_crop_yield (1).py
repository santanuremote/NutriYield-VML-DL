# -*- coding: utf-8 -*-
"""Exploring the impact of soil nutrients on crop yield

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HGcTMpHjLMUGH_-OuIPjvgG8CBmr9qEI
"""

pip install lightgbm catboost

# ─── Install Missing Libraries ───────────────────────────────────────────────
!pip install lightgbm catboost --quiet

# ─── Global Style ────────────────────────────────────────────────────────────
import matplotlib.pyplot as plt
plt.style.use('default')
plt.rcParams.update({
    "figure.facecolor": "white",
    "axes.facecolor": "white",
    "savefig.facecolor": "white",
    "font.family": "serif",
    "font.weight": "bold",
    "axes.titleweight": "bold",
    "axes.labelweight": "bold",
    "xtick.labelsize": 10,
    "ytick.labelsize": 10
})

# ─── Imports ─────────────────────────────────────────────────────────────────
import numpy as np
import pandas as pd
import seaborn as sns
from collections import Counter
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier


# ─── Load Dataset ────────────────────────────────────────────────────────────
data = pd.read_csv("/content/dataset.csv")
data.rename(columns={'ph': 'pH'}, inplace=True)

# ─── Exploratory Data Description ────────────────────────────────────────────
print("Shape:", data.shape)
print("Columns:", data.columns.tolist())
print("\nData Types:\n", data.dtypes)
print("\nMissing Values:\n", data.isnull().sum())
print("\nClass Distribution:\n", data['label'].value_counts())
print("\nStatistical Summary:\n", data.describe())
# ─── Count Plot ───────────────────────────────────────────────────────────────
plt.figure(figsize=(8, 5))
sns.countplot(x='label', data=data, palette='Paired')
plt.title("Crop Type Distribution")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ─── Correlation Matrix ───────────────────────────────────────────────────────
plt.figure(figsize=(12, 8))
sns.heatmap(data.corr(numeric_only=True), annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Matrix of Numerical Features")
plt.tight_layout()
plt.show()

# ─── Pairplot (Sampled for Speed) ─────────────────────────────────────────────
pairplot_data = data.select_dtypes(include='number').copy()
pairplot_data['label'] = data['label'].astype(str)
pairplot_sample = pairplot_data.sample(n=500, random_state=42)
sns.pairplot(pairplot_sample, hue='label', corner=True, diag_kind="hist", palette='viridis')
plt.suptitle("Pairwise Relationships Between Features (Sampled)", fontsize=16, y=1.02)
plt.show()

# ─── Boxplots ─────────────────────────────────────────────────────────────────
important_features = ['N', 'P', 'K', 'pH', 'EC', 'Mn', 'Zn']
plt.figure(figsize=(14, 10))
for i, col in enumerate(important_features, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x='label', y=col, hue='label', data=data, palette="Set2", legend=False)
    plt.title(f"{col} Distribution by Crop Type")
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ─── Violin Plots ─────────────────────────────────────────────────────────────
plt.figure(figsize=(14, 10))
for i, col in enumerate(important_features, 1):
    plt.subplot(3, 3, i)
    sns.violinplot(x='label', y=col, hue='label', data=data, palette="Set3", legend=False)
    plt.title(f"{col} Violin Plot by Crop Type")
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
# ─── Train-Test Split ────────────────────────────────────────────────────────
X = data.iloc[:, :11]
y = data['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y, random_state=42)
# ─── Split ───────────────────────────────────────────────────────────────────
X = data.iloc[:, :11]
y = data['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y, random_state=42)

# ─── Outlier Removal ─────────────────────────────────────────────────────────
def detect_outliers(df, n, features):
    outlier_indices = []
    for col in features:
        Q1 = np.percentile(df[col], 25)
        Q3 = np.percentile(df[col], 75)
        IQR = Q3 - Q1
        outlier_step = 1.5 * IQR
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index
        outlier_indices.extend(outlier_list_col)
    outlier_indices_counter = Counter(outlier_indices)
    multiple_outliers = [k for k, v in outlier_indices_counter.items() if v > n]
    return multiple_outliers

outliers = detect_outliers(X_train, 1, ['S'])
X_train.drop(outliers, axis=0, inplace=True)
y_train.drop(outliers, axis=0, inplace=True)
X_train.reset_index(drop=True, inplace=True)
y_train.reset_index(drop=True, inplace=True)

# ─── Skewness Correction ─────────────────────────────────────────────────────
for col in ['Mn', 'Zn', 'Fe', 'K', 'EC']:
    X_train[col] = stats.boxcox(X_train[col] + 1, lmbda=0.15)
    X_test[col] = stats.boxcox(X_test[col] + 1, lmbda=0.15)

# ─── Scaling ─────────────────────────────────────────────────────────────────
scaler = MinMaxScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)

# ─── Label Encoding ──────────────────────────────────────────────────────────
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)
# ─── Dimensionality Reduction: t-SNE & PCA ────────────────────────────────────
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare data
X = data.select_dtypes(include='number')
y = data['label'].astype(str)

# Normalize features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# ─── t-SNE Visualization ─────────────────────────────────────────────────────
tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto')
X_tsne = tsne.fit_transform(X_scaled)

plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette='Set2', s=60, edgecolor='black')
plt.title("t-SNE Visualization (2D)")
plt.xlabel("TSNE-1")
plt.ylabel("TSNE-2")
plt.legend(title='Label', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# ─── PCA Visualization ───────────────────────────────────────────────────────
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='Set1', s=60, edgecolor='black')
plt.title("PCA Visualization (2D)")
plt.xlabel(f"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)")
plt.ylabel(f"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)")
plt.legend(title='Label', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()
# ─── Train All Models ────────────────────────────────────────────────────────
rf = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)
ada = AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42)
svm = SVC(C=1, gamma='scale', kernel='rbf', probability=True, random_state=42)
knn = KNeighborsClassifier(n_neighbors=5)
log = LogisticRegression(max_iter=500, random_state=42)
lgbm = LGBMClassifier(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42)
cat = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, verbose=0, random_state=42)

rf.fit(X_train, y_train)
ada.fit(X_train, y_train)
svm.fit(X_train_scaled, y_train)
knn.fit(X_train_scaled, y_train)
log.fit(X_train_scaled, y_train)
lgbm.fit(X_train, y_train)
cat.fit(X_train, y_train)

# ─── Confusion Matrices ──────────────────────────────────────────────────────
models = [rf, ada, svm, knn, log, lgbm, cat]
names = ["Random Forest", "AdaBoost", "SVM", "KNN", "Logistic Regression", "LightGBM", "CatBoost"]

fig, axes = plt.subplots(3, 3, figsize=(20, 15))
axes = axes.flatten()

for idx, (model, name) in enumerate(zip(models, names)):
    if model in [svm, knn, log]:
        # These models were trained/predicted on scaled data
        y_pred = model.predict(X_test_scaled)
    else:
        # Other models trained/predicted on original or box-cox transformed data
        y_pred = model.predict(X_test)

    # Ensure y_pred is 1D for comparison, especially for CatBoost
    # CatBoost's predict can sometimes return a 2D array (n_samples, 1)
    if y_pred.ndim > 1:
        y_pred = y_pred.flatten()

    # Ensure y_pred and y_test have compatible types for comparison if needed
    # For consistency, convert y_test to a numpy array if y_pred is a numpy array
    if isinstance(y_pred, np.ndarray) and isinstance(y_test, pd.Series):
        y_test_comp = y_test.values
    else:
        y_test_comp = y_test

    acc = (y_pred == y_test_comp).mean() # Compare flattened y_pred with y_test_comp
    cm = confusion_matrix(y_test, y_pred, labels=le.classes_)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
    disp.plot(ax=axes[idx], cmap='plasma', xticks_rotation=45, colorbar=False)
    axes[idx].set_title(f"{name}\nAccuracy: {acc:.2f}", fontsize=12)

# Hide any unused subplots if the number of models is less than 9
for j in range(len(models), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# ─── INSTALL XGBOOST IF NEEDED ───────────────────────────────────────────────
!pip install xgboost --quiet

# ─── IMPORTS ─────────────────────────────────────────────────────────────────
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from collections import Counter

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score

from xgboost import XGBClassifier

# ─── LOAD DATA ───────────────────────────────────────────────────────────────
data = pd.read_csv("/content/dataset.csv")
data.rename(columns={'ph': 'pH'}, inplace=True)

# ─── SPLIT DATA ──────────────────────────────────────────────────────────────
X = data.iloc[:, :11]
y = data['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y, random_state=42)

# ─── OUTLIER REMOVAL ─────────────────────────────────────────────────────────
def detect_outliers(df, n, features):
    outlier_indices = []
    for col in features:
        Q1 = np.percentile(df[col], 25)
        Q3 = np.percentile(df[col], 75)
        IQR = Q3 - Q1
        outlier_step = 1.5 * IQR
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index
        outlier_indices.extend(outlier_list_col)
    outlier_indices_counter = Counter(outlier_indices)
    multiple_outliers = [k for k, v in outlier_indices_counter.items() if v > n]
    return multiple_outliers

outliers = detect_outliers(X_train, 1, ['S'])
X_train.drop(outliers, axis=0, inplace=True)
y_train.drop(outliers, axis=0, inplace=True)
X_train.reset_index(drop=True, inplace=True)
y_train.reset_index(drop=True, inplace=True)

# ─── BOX-COX TRANSFORMATION ──────────────────────────────────────────────────
for col in ['Mn', 'Zn', 'Fe', 'K', 'EC']:
    X_train[col] = stats.boxcox(X_train[col] + 1, lmbda=0.15)
    X_test[col] = stats.boxcox(X_test[col] + 1, lmbda=0.15)

# ─── SCALE FEATURES ──────────────────────────────────────────────────────────
scaler = MinMaxScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)

# ─── ENCODE LABELS ───────────────────────────────────────────────────────────
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

# ─── XGBOOST TRAINING ────────────────────────────────────────────────────────
xgb = XGBClassifier(
    n_estimators=200,
    learning_rate=0.1,
    max_depth=6,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)

xgb.fit(X_train_scaled, y_train_enc)

# ─── PREDICT AND EVALUATE ────────────────────────────────────────────────────
y_pred_enc = xgb.predict(X_test_scaled)
y_pred = le.inverse_transform(y_pred_enc)
acc = accuracy_score(y_test, y_pred)

# ─── CONFUSION MATRIX ────────────────────────────────────────────────────────
cm = confusion_matrix(y_test, y_pred, labels=le.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
disp.plot(cmap='plasma', xticks_rotation=45)
plt.title(f"XGBoost Confusion Matrix\nAccuracy: {acc:.2f}", fontsize=14)
plt.tight_layout()
plt.show()



